设计轮Q1，不是常见因佛伦斯那道题，是部署模型，应该是增加题库或者有些变化。题目是给你一个集群，里面几百个host，然后跟模型repo之间的带宽是10GB/s，模型几百G，每个host也有10GB带宽，然后之间也可以互相复制。一个基本思路是尽快把模型下载到一个host，然后host之间互相传播，最后传播路径会类似于一个树，这个方案还是不够快，然后我给了一个分块传播的想法，就是一个模型拆分成很多小块，然后每个小块好了就立马开始传播，这样可以更早得利用上其他host的带宽。

https://programhelp.net/vo/anthropic-interview-process-experience/?utm_source=chatgpt.com

第一题是设计 Inference API。
要实现一个能支持多模型、多版本的推理服务。
我分三步讲：

前端请求到 API Gateway；
Gateway 路由到不同的模型服务；
后端通过负载均衡分配 GPU 资源。
接着讨论了冷启动、模型缓存、请求超时重试这些细节。面试官重点问了“怎么做多版本管理”和“怎么应对模型加载慢的问题”。

第二题是 Prompt 交互平台。
类似一个小版 Claude 界面，让用户和模型对话。
我提到了 session 管理、历史记录、流式输出。面试官追问 token 计费逻辑，我补了用日志聚合和统计的方式去做账单分析。

第三题是 Batch Service。
题目要求设计一个批量任务处理系统。
我讲了 API 接收任务 → 存到队列 → worker 消费执行 → 存储结果的流程。
讲到失败重试机制时，我补充了死信队列（DLQ）的处理方案。
这一轮整体比较顺，但我在开头讲得太快，Programhelp 助攻提醒“先用一句话总结目标”，我重新组织开场后逻辑更清晰。
